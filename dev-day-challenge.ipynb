{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preview\n",
    "\n",
    "This notebook contains a challenge that uses the financebench data set (https://huggingface.co/datasets/PatronusAI/financebench).<br>\n",
    "**The general goal** is to get the best score on the benchmark evaluation (code below) :)\n",
    "\n",
    "This is a chellenging and realistic Financial Analyst test set where a question (can be complex!) and a context (a financial document, can be quite long!) are provided, and you're goal is to engineer an LLM flow, using Jamba, that solves as many of the questions on the test. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installs + imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas\n",
    "# %pip install ai21\n",
    "# %pip install ai21_tokenizer\n",
    "# %pip install OpenAI\n",
    "# %pip install python-docx\n",
    "# %pip install -U \"huggingface_hub[cli]\"\n",
    "# %pip install huggingface_hub\n",
    "\n",
    "\n",
    "# Import necessary modules\n",
    "from ai21 import AI21Client  # For AI21 client\n",
    "from ai21.models.chat.chat_message import SystemMessage, UserMessage, AssistantMessage  # For chat message models\n",
    "from ai21 import tokenizers\n",
    "from ai21_tokenizer import Jamba1_5Tokenizer\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed  # For concurrent execution\n",
    "import pandas as pd  # For data manipulation\n",
    "import time  # For measuring latency\n",
    "import os  # For file operations\n",
    "from docx import Document  # For reading docx content\n",
    "\n",
    "# For NVIDIA NIM using API key\n",
    "from openai import OpenAI \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# You must login to HuggingFace in order to use the tokeziner in this notebook. Please follow these instructions:\n",
    "# 1. Open a user in HuggingFace (or login)\n",
    "# 2. Request access to Jamba-1.5 models at https://huggingface.co/ai21labs/AI21-Jamba-1.5-Mini & https://huggingface.co/ai21labs/AI21-Jamba-1.5-Large\n",
    "# 3. Create a token with read access - https://huggingface.co/settings/tokens\n",
    "# Replace 'your_huggingface_token' with your actual token\n",
    "\n",
    "login(token=\"YOUR_HUGGINGFACE_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More info?\n",
    "**AI21 SDK documentation:** https://github.com/AI21Labs/ai21-python?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Available models\n",
    "MODEL_JAMBA_LARGE = \"jamba-1.5-large\"\n",
    "MODEL_JAMBA_MINI = \"jamba-1.5-mini\"\n",
    "MAX_INPUT_TOKENS = 150000\n",
    "TOKENIZER_MODEL_PATH = \"ai21labs/AI21-Jamba-1.5-Mini\"\n",
    "\n",
    "\n",
    "#Get a response from AI21 models, measure latency (Clue: heavily impacted by prompt size)\n",
    "def generate_response(messages,\n",
    "                      client,\n",
    "                      model:str = MODEL_JAMBA_MINI,\n",
    "                      max_tokens:int=1000,\n",
    "                      temperature=0.7,\n",
    "                      max_retries:int = 5,\n",
    "                      retry_delay:int = 1):\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            response = client.chat.completions.create(\n",
    "                messages=messages,\n",
    "                model=model,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "            )\n",
    "            end_time = time.time()\n",
    "            latency = end_time - start_time\n",
    "\n",
    "            return response.choices[0].message, latency\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt} Failed , Error generating response: {e}\")\n",
    "            time.sleep(retry_delay)\n",
    "\n",
    "    # start_time = time.time()\n",
    "    # response = client.chat.completions.create(\n",
    "    #     messages=messages,\n",
    "    #     model=model,\n",
    "    #     max_tokens=max_tokens,\n",
    "    #     temperature=temperature,\n",
    "    # )\n",
    "    # end_time = time.time()\n",
    "    # latency = end_time - start_time\n",
    "\n",
    "    return response.choices[0].message, latency\n",
    "\n",
    "#Generate messages - YOU CAN USE THIS FUNCTION\n",
    "def generate_messages(system_msg:str, user_msg:str, context:str, question:str):\n",
    "    # create defualt messages\n",
    "    messages = [\n",
    "        SystemMessage(content=system_msg, role=\"system\"),# Only use this if you want to set the system message, MUST BE FIRST\n",
    "        UserMessage(content=user_msg.format(context=context, question=question), role=\"user\")\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "def tokenize_and_truncate(text:str,\n",
    "                           max_tokens:int = MAX_INPUT_TOKENS,\n",
    "                           model_path:str = TOKENIZER_MODEL_PATH):\n",
    "    tokenizer = Jamba1_5Tokenizer(model_path=model_path)\n",
    "    encoded = tokenizer.encode(text)\n",
    "    return tokenizer.decode(encoded[:max_tokens]) if max_tokens < len(encoded) else text\n",
    "\n",
    "def read_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            \n",
    "            #NOTE:We trunicate the file content 150K tokens,\n",
    "            #     as answer resides in the first 150K tokens\n",
    "            return tokenize_and_truncate(content)\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "\n",
    "def append_md_file_contents(df: pd.DataFrame, folder_path: str) -> pd.DataFrame:\n",
    "    # Create new columns to store file contents\n",
    "    df['md_format'] = None\n",
    "\n",
    "    # Iterate over each row in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        source_document = row['source_document']\n",
    "    \n",
    "        # Construct file paths for each format\n",
    "        md_path = os.path.join(folder_path, f\"{source_document}.md\")\n",
    "        \n",
    "        # Read and append the contents\n",
    "        df.at[index, 'md_format'] = read_file(md_path)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "fin_bench_df = pd.read_csv(\"/Users/orishapira/Desktop/NVIDIA-AI21-dev-day-challenge/financebench_question_answer_doc_dataset.csv\")\n",
    "\n",
    "#Add financial docs content from files to the DataFrame - **NOTE:in Markdown format only**\n",
    "folder_path = '/Users/orishapira/Desktop/nvidia-dev-challenge/data'\n",
    "fin_bench_df = append_md_file_contents(fin_bench_df, folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Data columns are\n",
    "**1. question:** contains the question to be answered<br>\n",
    "**2. gold_answer:** the true and correct answer to the question<br>\n",
    "**3. source_document:** the document containing the answer<br>\n",
    "**4. md_format:** the parsed relevant data from the .md version of the source_document<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>gold_answer</th>\n",
       "      <th>source_document</th>\n",
       "      <th>md_format</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the FY2018 capital expenditure amount ...</td>\n",
       "      <td>$1577.00</td>\n",
       "      <td>3M_2018_10K</td>\n",
       "      <td>low\\n\\n**UNITED STATES**\\n\\n**SECURITIES AND E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Assume that you are a public equities analyst....</td>\n",
       "      <td>$8.70</td>\n",
       "      <td>3M_2018_10K</td>\n",
       "      <td>low\\n\\n**UNITED STATES**\\n\\n**SECURITIES AND E...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question gold_answer  \\\n",
       "0  What is the FY2018 capital expenditure amount ...    $1577.00   \n",
       "1  Assume that you are a public equities analyst....       $8.70   \n",
       "\n",
       "  source_document                                          md_format  \n",
       "0     3M_2018_10K  low\\n\\n**UNITED STATES**\\n\\n**SECURITIES AND E...  \n",
       "1     3M_2018_10K  low\\n\\n**UNITED STATES**\\n\\n**SECURITIES AND E...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the data\n",
    "fin_bench_df.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Create clients and naive prompts\n",
    "\n",
    "**You can choose to work with one of 3 options,<br>**\n",
    "Pass the relevant client to generate_response method according to your preference:<br><br>\n",
    "**1 AI21 Studio** - If you are using AI21 studio, pass the ai21_client<br><br>\n",
    "**2. Jamba NIM** -  pass the nvidia client. API key can be achieved in https://build.nvidia.com/ai21labs/jamba-1_5-large-instruct?api_key=true when pressing \"Get API key\", on the top right corner of the Python snippet. Email is required to register to NVIDIA developers programs.<br><br>\n",
    "**3 Jamba NIM  via Langchain** - in Langchain ChatNVIDIA module. https://python.langchain.com/docs/integrations/chat/nvidia_ai_endpoints/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#create the client and defualt messages\n",
    "AI21_API_KEY = 'YOUR_AI21_API_KEY'\n",
    "NVIDIA_NIM_API_KEY = \"$<YOUR_NVIDIA_API_KEY>\"\n",
    "\n",
    "#NOTE: you need the ai21_client for the evaluation so don't delete it\n",
    "ai21_client = AI21Client(api_key=AI21_API_KEY)\n",
    "nvidia_client = OpenAI(\n",
    "  base_url = \"https://integrate.api.nvidia.com/v1\",\n",
    "  api_key = NVIDIA_NIM_API_KEY, \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "default_system_msg = \"You are a financial assistant\"\n",
    "default_user_msg   = \"\"\"based on the following context: {context}\\nanswer the following question: {question}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Challenge\n",
    "\n",
    "Your task is to modify the code below to achive the best results possible!<br>\n",
    "Think outside of the box! you can change the defualt prompts, use langchain, add steps costum to the flow or what ever you can think of.<br>\n",
    "<br>You may:<br>\n",
    "* Change the helper functions (except *generate_messages()* )\n",
    "* Add helper functions\n",
    "* Use the non .md files provided under data folder\n",
    "* Use any external library you see fit.\n",
    "* (pre/post) Process the data.\n",
    "* Use chaining.\n",
    "* Do what ever makes it work better... \n",
    "\n",
    "\n",
    "<br>You may NOT:<br>\n",
    "* Modify *generate_messages()*\n",
    "\n",
    "**IMPORTANT Suggetion**<br>\n",
    "* Use a sub-set( a small sample of the data) to test yourself when making changes. As running and evaluating the full data set may take a while...<br>\n",
    "(Approx. 10 min for getting the answers, Y min for evaluation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace/Add your code here\n",
    "def generate_answers(df: pd.DataFrame, \n",
    "                     client = None,\n",
    "                     model: str = MODEL_JAMBA_MINI,\n",
    "                     max_tokens: int = 1000,\n",
    "                     temperature: float = 0.7,\n",
    "                     format: str = \"md_format\") -> pd.DataFrame:\n",
    "    \n",
    "    if type(client)==OpenAI : # change model name if using nvidia NIM (OpenAI client)\n",
    "        model = \"ai21labs/{}-instruct\".format(model)\n",
    "            \n",
    "    def process_row(row):\n",
    "        messages = generate_messages(system_msg=default_system_msg,\n",
    "                                     user_msg=default_user_msg,\n",
    "                                     context=row[format],\n",
    "                                     question=row['question'])\n",
    "        answer, latency = generate_response(messages, client, model, max_tokens, temperature)\n",
    "        return pd.Series({'answer': answer.content, 'latency': latency})\n",
    "\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Apply the process_row function to each row\n",
    "    result = result_df.progress_apply(process_row, axis=1)\n",
    "    \n",
    "    # Assign the results to new columns\n",
    "    result_df[['model_answer', 'latency']] = result\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: If you are using NVIDIA NIM, replace the client with nvidia_client\n",
    "answers_df = generate_answers(df=fin_bench_df, client=ai21_client)\n",
    "\n",
    "answers_df.to_csv( \"model_answers.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output file should be a csv containing a \"model_answer\" column and a \"gold_answer\" column. The evaluation code below reads the file and runs a JudgeLM on the results to evaluate whether they are correct or not.\n",
    "\n",
    "The scores is the average of the JudgeLM score across your model predictions.\n",
    "\n",
    "***Baseline results for the provided configuration are around 55%***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "jlm_prompt_template = \"\"\"You are tasked with evaluating the response of a question-answering model. \n",
    "You will be given a correct reference answer and the model's prediction, and will need to judge its correctness. The model's prediction may contain reasoning steps and explanations that go beyond the simple reference answer provided. You should judge the actual information content of the model's answer and check whether it aligns with the reference.\n",
    "Your response must be a valid json dictionary with the key \"label\" and a value that's either 0 (incorrect) or 1 (correct).\n",
    "\n",
    "The following are the reference answer and model prediction:\n",
    "\n",
    "Model prediction:\n",
    "{model_answer}\n",
    "\n",
    "Reference answer:\n",
    "{gold_answer}\"\"\"\n",
    "\n",
    "\n",
    "def evaluate_answer(model_answer:str,\n",
    "                    gold_answer:str,\n",
    "                    client,\n",
    "                    max_retries:int = 5,\n",
    "                    retry_delay:int = 1):\n",
    "    try:\n",
    "        messages = [\n",
    "            UserMessage(content=jlm_prompt_template.format(model_answer=model_answer, gold_answer=gold_answer), role=\"user\")\n",
    "        ]\n",
    "\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                res = client.chat.completions.create(\n",
    "                    messages=messages,\n",
    "                    model=\"jamba-1.5-large\",\n",
    "                    max_tokens=50,\n",
    "                    temperature=0.01,\n",
    "                )\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error in JLM: {e}\")\n",
    "            \n",
    "            time.sleep(retry_delay)\n",
    "        res = res.choices[0].message.content\n",
    "\n",
    "        try:\n",
    "            res = json.loads(res)['label']\n",
    "            return res\n",
    "        except json.decoder.JSONDecodeError:\n",
    "            return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in JLM: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def evaluate_results(df: pd.DataFrame, client: AI21Client, verbose=True):\n",
    "    assert \"model_answer\" in df, \"model_answer field must be part of the results data\"\n",
    "    assert \"gold_answer\" in df, \"gold_answer field must be part of the results data\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Evaluating {len(df)} answers...\")\n",
    "    df['jlm_score'] = df.progress_apply(lambda row: evaluate_answer(row['model_answer'], row['gold_answer'], client), axis=1)\n",
    "    if verbose:\n",
    "        print(f\"Evaluated {len(df)} answers, final score: {df['jlm_score'].mean()}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run your evaluation data:\n",
    "df = pd.read_csv(\"<YOUR_PATH_HERE>\")\n",
    "results_df = evaluate_results(answers_df, ai21_client, verbose=True)\n",
    "results_df.to_csv(\"<YOUR_OUTPUT_PATH_HERE>\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
