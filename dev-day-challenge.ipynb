{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preview\n",
    "\n",
    "This notebook contains a challenge that uses the financebench data set (https://huggingface.co/datasets/PatronusAI/financebench).<br>\n",
    "**The general goal** is to get the best results :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installs + imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas\n",
    "# %pip install ai21\n",
    "# %pip install ai21_tokenizer\n",
    "# %pip install python-docx\n",
    "# %pip install -U \"huggingface_hub[cli]\"\n",
    "\n",
    "\n",
    "# Import necessary modules\n",
    "from ai21 import AI21Client  # For AI21 client\n",
    "from ai21.models.chat.chat_message import SystemMessage, UserMessage, AssistantMessage  # For chat message models\n",
    "from ai21 import tokenizers\n",
    "from ai21_tokenizer import Jamba1_5Tokenizer\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed  # For concurrent execution\n",
    "import pandas as pd  # For data manipulation\n",
    "import time  # For measuring latency\n",
    "import os  # For file operations\n",
    "from docx import Document  # For reading docx content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - ROI to write a guide for HF token\n",
    "\n",
    "\n",
    "#Run this in terminal\n",
    "huggingface-cli -login\n",
    "\n",
    "#using this token\n",
    "HF_TOKEN = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More info?\n",
    "**AI21 SDK documentation:** https://github.com/AI21Labs/ai21-python?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Available models\n",
    "MODEL_JAMBA_LARGE = \"jamba-1.5-large\"\n",
    "MODEL_JAMBA_MINI = \"jamba-1.5-mini\"\n",
    "MAX_INPUT_TOKENS = 150000\n",
    "TOKENIZER_MODEL_PATH = \"ai21labs/AI21-Jamba-1.5-Mini\"\n",
    "\n",
    "\n",
    "#Get a response from AI21 models, measure latency (Clue: heavily impacted by prompt size)\n",
    "def generate_response(messages,\n",
    "                      client,\n",
    "                      model:str = MODEL_JAMBA_MINI,\n",
    "                      max_tokens:int=1000,\n",
    "                      temperature=0.7\n",
    "                      ):\n",
    "    start_time = time.time()\n",
    "    response = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=model,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    latency = end_time - start_time\n",
    "\n",
    "    return response.choices[0].message, latency\n",
    "\n",
    "#Generate messages - YOU CAN USE THIS FUNCTION\n",
    "def generate_messages(system_msg:str, user_msg:str, context:str, question:str):\n",
    "    # create defualt messages\n",
    "    messages = [\n",
    "        SystemMessage(content=system_msg, role=\"system\"),# Only use this if you want to set the system message, MUST BE FIRST\n",
    "        UserMessage(content=user_msg.format(context=context, question=question), role=\"user\")\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "def tokenize_and_truncate(text:str,\n",
    "                           max_tokens:int = MAX_INPUT_TOKENS,\n",
    "                           model_path:str = TOKENIZER_MODEL_PATH):\n",
    "    tokenizer = Jamba1_5Tokenizer(model_path=model_path)\n",
    "    encoded = tokenizer.encode(text)\n",
    "    return tokenizer.decode(encoded[:max_tokens]) if max_tokens < len(encoded) else text\n",
    "\n",
    "def read_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            \n",
    "            #NOTE:We trunicate the file content 150K tokens,\n",
    "            #     as answer resides in the first 150K tokens\n",
    "            return tokenize_and_truncate(content)\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "\n",
    "def append_md_file_contents(df: pd.DataFrame, folder_path: str) -> pd.DataFrame:\n",
    "    # Create new columns to store file contents\n",
    "    df['md_format'] = None\n",
    "\n",
    "    # Iterate over each row in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        source_document = row['source_document']\n",
    "    \n",
    "        # Construct file paths for each format\n",
    "        md_path = os.path.join(folder_path, f\"{source_document}.md\")\n",
    "        \n",
    "        # Read and append the contents\n",
    "        df.at[index, 'md_format'] = read_file(md_path)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "fin_bench_df = pd.read_csv(\"/Users/orishapira/Desktop/NVIDIA-AI21-dev-day-challenge/financebench_question_answer_doc_dataset.csv\")\n",
    "\n",
    "#Add financial docs content from files to the DataFrame - **NOTE:in Markdown format only**\n",
    "folder_path = '/Users/orishapira/Desktop/nvidia-dev-challenge/data'\n",
    "fin_bench_df = append_md_file_contents(out, folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Data columns are\n",
    "**1. question:** contains the question to be answered<br>\n",
    "**2. gold_answer:** the true and correct answer to the question<br>\n",
    "**3. source_document:** the document containing the answer<br>\n",
    "**4. md_format:** the parsed relevant data from the .md version of the source_document<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>gold_answer</th>\n",
       "      <th>source_document</th>\n",
       "      <th>md_format</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the FY2018 capital expenditure amount ...</td>\n",
       "      <td>$1577.00</td>\n",
       "      <td>3M_2018_10K</td>\n",
       "      <td>low\\n\\n**UNITED STATES**\\n\\n**SECURITIES AND E...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question gold_answer  \\\n",
       "0  What is the FY2018 capital expenditure amount ...    $1577.00   \n",
       "\n",
       "  source_document                                          md_format  \n",
       "0     3M_2018_10K  low\\n\\n**UNITED STATES**\\n\\n**SECURITIES AND E...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the data\n",
    "fin_bench_df.head(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Create client and naive prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#create the client and defualt messages\n",
    "#TODO: DELETE THIS#\n",
    "ORISH_API_KEY = 'bRCzorjq8RWyl75MZKqeNGGPy8A40J4b'\n",
    "# client = AI21Client(api_key=\"YOUR_API_KEY\")\n",
    "\n",
    "client = AI21Client(api_key=ORISH_API_KEY)#TODO: DELETE THIS#\n",
    "default_system_msg = \"You are a financial assistant\"\n",
    "default_user_msg   = \"\"\"based on the following context: {context}\\nanswer the following question: {question}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Challenge\n",
    "\n",
    "Your task is to modify the code below to achive the best results possible!<br>\n",
    "Think outside of the box! you can change the defualt prompts, use langchain, add steps costum to the flow or what ever you can think of.<br>\n",
    "<br>You may:<br>\n",
    "* change the helper functions (except *generate_messages()* )\n",
    "* add helper functions\n",
    "* use the non .md files provided under data folder\n",
    "*\n",
    "\n",
    "\n",
    "<br>You may NOT:<br>\n",
    "* change *generate_messages()*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace/Add your code here\n",
    "def generate_answers(df: pd.DataFrame, \n",
    "                     client: AI21Client = None,\n",
    "                     model: str = MODEL_JAMBA_MINI,\n",
    "                     max_tokens: int = 1000,\n",
    "                     temperature: float = 0.7,\n",
    "                     format: str = \"md_format\") -> pd.DataFrame:\n",
    "    \n",
    "    def process_row(row):\n",
    "        messages = generate_messages(system_msg=default_system_msg,\n",
    "                                     user_msg=default_user_msg,\n",
    "                                     context=row[format],\n",
    "                                     question=row['question'])\n",
    "        answer, latency = generate_response(messages, client, model, max_tokens, temperature)\n",
    "        return pd.Series({'answer': answer.content, 'latency': latency})\n",
    "\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Apply the process_row function to each row\n",
    "    result = result_df.apply(process_row, axis=1)\n",
    "    \n",
    "    # Assign the results to new columns\n",
    "    result_df[['model_answer', 'latency']] = result\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_df = generate_answers(df=fin_bench_df, client=client)\n",
    "answers_df.to_csv( \"model_answers.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output file should be a csv containing a \"model_answer\" column and a \"gold_answer\" column. The evaluation code below reads the file and runs a JudgeLM on the results to evaluate whether they are correct or not.\n",
    "\n",
    "The scores is the average of the JudgeLM score across your model predictions.\n",
    "\n",
    "***Baseline results for the provided configuration are around ___TODO__%***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "jlm_prompt_template = \"\"\"You are tasked with evaluating the response of a question-answering model. \n",
    "You will be given a correct reference answer and the model's prediction, and will need to judge its correctness. The model's prediction may contain reasoning steps and explanations that go beyond the simple reference answer provided. You should judge the actual information content of the model's answer and check whether it aligns with the reference.\n",
    "Your response must be a valid json dictionary with the key \"label\" and a value that's either 0 (incorrect) or 1 (correct).\n",
    "\n",
    "The following are the reference answer and model prediction:\n",
    "\n",
    "Model prediction:\n",
    "{model_answer}\n",
    "\n",
    "Reference answer:\n",
    "{gold_answer}\"\"\"\n",
    "\n",
    "\n",
    "def evaluate_answer(model_answer, gold_answer, client):\n",
    "    try:\n",
    "        messages = [\n",
    "            UserMessage(content=jlm_prompt_template.format(model_answer=model_answer, gold_answer=gold_answer), role=\"user\")\n",
    "        ]\n",
    "\n",
    "        res = client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            model=\"jamba-1.5-large\",\n",
    "            max_tokens=50,\n",
    "            temperature=0.01,\n",
    "        )\n",
    "        res = res.choices[0].message.content\n",
    "\n",
    "        try:\n",
    "            res = json.loads(res)['label']\n",
    "            return res\n",
    "        except json.decoder.JSONDecodeError:\n",
    "            return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in JLM: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def evaluate_results(df: pd.DataFrame, client: AI21Client, verbose=True):\n",
    "    assert \"model_answer\" in df, \"model_answer field must be part of the results data\"\n",
    "    assert \"gold_answer\" in df, \"gold_answer field must be part of the results data\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Evaluating {len(df)} answers...\")\n",
    "    df['jlm_score'] = df.progress_apply(lambda row: evaluate_answer(row['model_answer'], row['gold_answer'], client), axis=1)\n",
    "    if verbose:\n",
    "        print(f\"Evaluated {len(df)} answers, final score: {df['jlm_score'].mean()}\")\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run your evaluation data:\n",
    "df = pd.read_csv(\"<YOUR_PATH_HERE>\")\n",
    "results_df = evaluate_results(df, client, verbose=True)\n",
    "results_df.to_csv(\"<YOUR_OUTPUT_PATH_HERE>\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
